{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "melk.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMEGKumpaFbZgQ64yBOGZ3v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bartianbosch/Melk/blob/Bartian/melk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrZIkT-ozHkb",
        "outputId": "8a057a19-409d-481c-d6a4-a6fdc4e60848"
      },
      "source": [
        "!ls ../content/spark-2.4.7-bin-hadoop2.7/python/lib"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "py4j-0.10.7-src.zip  PY4J_LICENSE.txt  pyspark.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLHOuzts0Wqo"
      },
      "source": [
        "# !apt-get update\n",
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# !wget -q http://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "# !tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "# !pip install -q findspark\n",
        "# !pip install -q pyspark\n",
        "\n",
        "# import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "# os.environ[\"PYTHON\"] = os.environ[\"SPARK_HOME\"] + \"/python\"\n",
        "# import findspark\n",
        "# findspark.init()\n",
        "# !os.environ[\"SPARK_HOME\"]/python;os.environ[\"SPARK_HOME\"]/python/lib/py4j-0.10.7-src.zip:/python\n",
        "\n",
        "!export SPARK_HOME=/content/spark-2.4.7-bin-hadoop2.7\n",
        "!export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH\n",
        "!export PATH=$SPARK_HOME/bin:$SPARK_HOME/python:$PATH"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "VZ2mPRZSHiRl",
        "outputId": "97ad5095-2c09-47e5-aec8-d97a3c9f8723"
      },
      "source": [
        "# To initialize Apache Spark, run this cell\n",
        "# !apt-get update\n",
        "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# !wget -q http://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "# !tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "# !pip install -q findspark\n",
        "# !pip install -q pyspark\n",
        "# !/content/spark-2.4.7-bin-hadoop2.7/python;/content/spark-2.4.7-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip:/etc/python\n",
        "\n",
        "# import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark import SparkFiles\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import types\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.regression import LinearRegressionModel\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns \n",
        "\n",
        "# Initialise stuff\n",
        "\n",
        "\n",
        "sp = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = SparkContext.getOrCreate()\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "plt.close('all')\n",
        "plt.rcParams['figure.figsize'] = [30, 20]\n",
        "\n",
        "# milk_data = spark.read.csv('/content/gdrive/MyDrive/Big_Data_Groupwork/ff_dairy_v1.csv', header=True, inferSchema = True)\n",
        "url = 'https://raw.githubusercontent.com/bartianbosch/Melk/main/ff_dairy_v1_preprocessed.csv'\n",
        "fileName = url.split('/')[-1]\n",
        "spark.sparkContext.addFile(url)\n",
        "milk_data = spark.read.csv(SparkFiles.get(fileName), header=True)\n",
        "\n",
        "def fixType(datasetName, colName, dataType):\n",
        "  \"\"\"Takes a dataset and returns the dataset with the given column set to the given dataype\"\"\"\n",
        "  return datasetName.withColumn(colName, F.col(colName).cast(dataType))\n",
        "\n",
        "# Preparing Lists with the colum names and which type they need to be\n",
        "allColumns = ['Cow number', 'Date', 'Barn', 'Barn part', 'Treatment', 'Lactation number', 'Days in milk', 'Day number', 'Week', 'Period (VP = preperiod, HP = trial period)', 'Period part (VP = preperiod, HP = trial period)', 'Intake basal ration (kg dm)', 'Intake total ration (kg dm)', 'Water intake (kg)', 'Concentrate intake (kg)', 'Weight (kg)', 'BCS', 'Backfat Thickness', 'Milk yield (kg)', 'Fat%', 'Protein%', 'Urea (mg/kg)', 'SCC (*1000/ml)', 'Lactose%', 'Avg Temperature (degC)', 'Air speed', 'Max temperature (degC)', 'Rumination time (min/day)', 'Avg pH', 'Avg Rumen Temperature ', 'Time pH<5,8 (h)', 'Manure consistency (1=fluid, 5= hard)', 'Manure fibre score (1=short, 5 = long)', 'Number of kernels', 'Manure sieving residu (%)', 'Manure dm (g/kg OM)', 'Manure starch (g/kg dm)', 'Urin-pH', 'Intake pre-period (kg dm)', 'Milk yield prepreriod (kg)', 'Fat% preperiod', 'Protein% preperiod', 'Fat preperiod (kg)', 'Protein preperiod (kg)', 'ECM preperiod (kg)', 'ECM (kg)', 'Crude fiber (g/day)', 'Sugar (g/day)', 'Crude protein (g/day)', 'Crude fat (g/day)', 'Starch (g/day)', 'Ca (g/day)', 'P (g/day)', 'Na (g/day)', 'Cl (g/day)', 'K (g/day)', 'Mg (g/day)', 'nXP (g/day)', 'RNB (g/day)', 'MELK (g/day)', 'NEL (g/day)', 'WDE (g/day)', 'EKB (g/day)', 'WDS (g/day)', 'SPK (g/day)', 'TPK (g/day)', 'SPE (g/day)', 'TPE (g/day)', 'plusI (/day)', 'WI (/day)', 'GP (g/day)', 'Crude fiber (g/kg dm)', 'Sugar (g/kg dm)', 'Crude protein (g/kg dm)', 'Crude fat (g/kg dm)', 'Starch (g/kg dm)', 'Ca (g/kg dm)', 'P (g/kg dm)', 'Na (g/kg dm)', 'Cl (g/kg dm)', 'K (g/kg dm)', 'Mg (g/kg dm)', 'nXP (g/kg dm)', 'RNB (g/kg dm)', 'MELK (/kg dm)', 'NEL (MJ/kg dm)', 'WDE (g/kg dm)', 'EKB (g/kg dm)', 'WDS (g/kg dm)', 'SPK (g/kg dm)', 'TPK (g/kg dm)', 'SPE (g/kg dm)', 'TPE (g/kg dm)', 'plusI (/kg dm)', 'WI (/kg dm)', 'GP (g/kg dm)']\n",
        "doubleColumns = ['Intake basal ration (kg dm)', 'Intake total ration (kg dm)', 'Water intake (kg)', 'Concentrate intake (kg)', 'Weight (kg)', 'BCS', 'Milk yield (kg)', 'Fat%', 'Protein%', 'Lactose%', 'Avg Temperature (degC)', 'Air speed', 'Max temperature (degC)', 'Avg pH', 'Avg Rumen Temperature ', 'Time pH<5,8 (h)', 'Number of kernels', 'Manure sieving residu (%)', 'Manure starch (g/kg dm)', 'Urin-pH', 'Intake pre-period (kg dm)', 'Milk yield prepreriod (kg)', 'Fat% preperiod', 'Protein% preperiod', 'Fat preperiod (kg)', 'Protein preperiod (kg)', 'ECM preperiod (kg)', 'ECM (kg)']\n",
        "intColumns = ['Cow number', 'Barn', 'Barn part', 'Lactation number', 'Days in milk', 'Day number', 'Week', 'Backfat Thickness', 'Urea (mg/kg)', 'SCC (*1000/ml)', 'Rumination time (min/day)', 'Manure consistency (1=fluid, 5= hard)', 'Manure fibre score (1=short, 5 = long)', 'Manure dm (g/kg OM)', 'Crude fiber (g/day)', 'Sugar (g/day)', 'Crude protein (g/day)', 'Crude fat (g/day)', 'Starch (g/day)', 'Ca (g/day)', 'P (g/day)', 'Na (g/day)', 'Cl (g/day)', 'K (g/day)', 'Mg (g/day)', 'nXP (g/day)', 'RNB (g/day)', 'MELK (g/day)', 'NEL (g/day)', 'WDE (g/day)', 'EKB (g/day)', 'WDS (g/day)', 'SPK (g/day)', 'TPK (g/day)', 'SPE (g/day)', 'TPE (g/day)', 'plusI (/day)', 'WI (/day)', 'GP (g/day)', 'Crude fiber (g/kg dm)', 'Sugar (g/kg dm)', 'Crude protein (g/kg dm)', 'Crude fat (g/kg dm)', 'Starch (g/kg dm)', 'Ca (g/kg dm)', 'P (g/kg dm)', 'Na (g/kg dm)', 'Cl (g/kg dm)', 'K (g/kg dm)', 'Mg (g/kg dm)', 'nXP (g/kg dm)', 'RNB (g/kg dm)', 'MELK (/kg dm)', 'NEL (MJ/kg dm)', 'WDE (g/kg dm)', 'EKB (g/kg dm)', 'WDS (g/kg dm)', 'SPK (g/kg dm)', 'TPK (g/kg dm)', 'SPE (g/kg dm)', 'TPE (g/kg dm)', 'plusI (/kg dm)', 'WI (/kg dm)', 'GP (g/kg dm)']\n",
        "stringColumn = ['Treatment', 'Period (VP = preperiod, HP = trial period)', 'Period part (VP = preperiod, HP = trial period)']\n",
        "milk_rdd2 = milk_data\n",
        "\n",
        "# Setting the columns to the correct type\n",
        "for colName in doubleColumns:\n",
        "  milk_rdd2 = fixType(milk_rdd2, colName, \"double\")\n",
        "for colName in intColumns:\n",
        "  milk_rdd2 = fixType(milk_rdd2, colName, \"int\")\n",
        "milk_data = milk_rdd2.withColumn(\"Date\", F.col(\"Date\").cast(\"date\"))\n",
        "\n",
        "# milk_data.printSchema()\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-0cf3593d6977>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m--> 136\u001b[0;31m                           conf, jsc, profiler_cls)\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;31m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# data via a socket.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# scala's mangled names w/ $ in them require special treatment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encryption_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misEncryptionEnabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpythonExec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PYSPARK_PYTHON\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             raise Py4JError(\n\u001b[0;32m-> 1531\u001b[0;31m                 \"{0}.{1} does not exist in the JVM\".format(self._fqn, name))\n\u001b[0m\u001b[1;32m   1532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JError\u001b[0m: org.apache.spark.api.python.PythonUtils.isEncryptionEnabled does not exist in the JVM"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZoKULzabvt4"
      },
      "source": [
        "milk_data.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "odJJ8ZTnIEqt",
        "outputId": "5b2b8f55-cd7f-4ec5-f570-d775f4bee8fc"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\n",
        "\n",
        "features_cols = ['Crude fiber (g/day)', 'Sugar (g/day)', 'Crude protein (g/day)', 'Crude fat (g/day)', 'Starch (g/day)', \n",
        "                             'Ca (g/day)', 'P (g/day)', 'Na (g/day)', 'Cl (g/day)', 'K (g/day)', 'Mg (g/day)']\n",
        "# features_cols = ['Cow number', 'Date', 'Barn', 'Barn part', 'Treatment', 'Lactation number', 'Days in milk', 'Day number', 'Week', 'Period (VP = preperiod, HP = trial period)', 'Period part (VP = preperiod, HP = trial period)', 'Intake basal ration (kg dm)', 'Intake total ration (kg dm)', 'Water intake (kg)', 'Concentrate intake (kg)', 'Weight (kg)', 'BCS', 'Backfat Thickness', 'Milk yield (kg)', 'Fat%', 'Protein%', 'Urea (mg/kg)', 'SCC (*1000/ml)', 'Lactose%', 'Avg Temperature (degC)', 'Air speed', 'Max temperature (degC)', 'Rumination time (min/day)', 'Avg pH', 'Avg Rumen Temperature ', 'Time pH<5,8 (h)', 'Manure consistency (1=fluid, 5= hard)', 'Manure fibre score (1=short, 5 = long)', 'Number of kernels', 'Manure sieving residu (%)', 'Manure dm (g/kg OM)', 'Manure starch (g/kg dm)', 'Urin-pH', 'Intake pre-period (kg dm)', 'Milk yield prepreriod (kg)', 'Fat% preperiod', 'Protein% preperiod', 'Fat preperiod (kg)', 'Protein preperiod (kg)', 'ECM preperiod (kg)', 'ECM (kg)', 'Crude fiber (g/day)', 'Sugar (g/day)', 'Crude protein (g/day)', 'Crude fat (g/day)', 'Starch (g/day)', 'Ca (g/day)', 'P (g/day)', 'Na (g/day)', 'Cl (g/day)', 'K (g/day)', 'Mg (g/day)', 'nXP (g/day)', 'RNB (g/day)', 'MELK (g/day)', 'NEL (g/day)', 'WDE (g/day)', 'EKB (g/day)', 'WDS (g/day)', 'SPK (g/day)', 'TPK (g/day)', 'SPE (g/day)', 'TPE (g/day)', 'plusI (/day)', 'WI (/day)', 'GP (g/day)', 'Crude fiber (g/kg dm)', 'Sugar (g/kg dm)', 'Crude protein (g/kg dm)', 'Crude fat (g/kg dm)', 'Starch (g/kg dm)', 'Ca (g/kg dm)', 'P (g/kg dm)', 'Na (g/kg dm)', 'Cl (g/kg dm)', 'K (g/kg dm)', 'Mg (g/kg dm)', 'nXP (g/kg dm)', 'RNB (g/kg dm)', 'MELK (/kg dm)', 'NEL (MJ/kg dm)', 'WDE (g/kg dm)', 'EKB (g/kg dm)', 'WDS (g/kg dm)', 'SPK (g/kg dm)', 'TPK (g/kg dm)', 'SPE (g/kg dm)', 'TPE (g/kg dm)', 'plusI (/kg dm)', 'WI (/kg dm)', 'GP (g/kg dm)']\n",
        "\n",
        "predictionColName = 'predProtein'\n",
        "labelColName = \"Milk yield (kg)\"\n",
        "\n",
        "\n",
        "milk_data = milk_data.select(allColumns).dropna(how='any')\n",
        "\n",
        "# Split dataset into test and training data\n",
        "test_data, train_data = milk_data.randomSplit([0.8, 0.2], 0)\n",
        "\n",
        "# Stages\n",
        "# index_strings = [StringIndexer(inputCol= col, outputCol= f'{col}_index') for col in stringColumn]\n",
        "\n",
        "# encoder = OneHotEncoderEstimator(inputCols=[string_indexer.getOutputCol() for string_indexer in index_strings], \n",
        "                                #  outputCols= [f'{col}_encoded' for col in stringColumn])\n",
        "\n",
        "vectorizer = VectorAssembler(\n",
        "    # inputCols = doubleColumns + intColumns + [f'{col}_encoded' for col in stringColumn],\n",
        "    inputCols = [\"labelColName\"] + features_cols,\n",
        "    outputCol = \"features\")\n",
        "\n",
        "lr = LinearRegression(featuresCol = 'features', labelCol=labelColName, predictionCol = predictionColName)\n",
        "\n",
        "# Create pipeline\n",
        "lr_pipeline = Pipeline(stages = [vectorizer] + [lr]) #index_strings + [encoder] + \n",
        "\n",
        "# Fit pipeline\n",
        "lr_model = lr_pipeline.fit(train_data)\n",
        "\n",
        "# vectorized = vectorizer.transform(nutrients) #.select('features', 'Protein%')\n",
        "\n",
        "\n",
        "# lrModel = lr.fit(train_data)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6391ee2e679b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearRegressionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneHotEncoderEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m features_cols = ['Crude fiber (g/day)', 'Sugar (g/day)', 'Crude protein (g/day)', 'Crude fat (g/day)', 'Starch (g/day)', \n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'OneHotEncoderEstimator'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcD5Bv5TXilt",
        "outputId": "0024d714-0d1f-4232-e9df-6e1b0c53091c"
      },
      "source": [
        "# Inspecting the results of the previous model\n",
        "\n",
        "# The coefficients (i.e., weights) are as follows:\n",
        "weights = lrModel.coefficients\n",
        "\n",
        "# Print coefficients \n",
        "list(zip(features_cols, weights))\n",
        "print(list(zip(features_cols, weights)))\n",
        " \n",
        " # Print the intercept\n",
        "print(\"Intercept: \",lrModel.intercept)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Crude fiber (g/day)', -0.003975544125964483), ('Sugar (g/day)', 0.015414138197454961), ('Crude protein (g/day)', -0.02881282320707079), ('Crude fat (g/day)', 0.040742235016491345), ('Starch (g/day)', 0.009536090850106057), ('Ca (g/day)', 0.049188047500887025), ('P (g/day)', -0.02280844594882234), ('Na (g/day)', 0.02001380902446119), ('Cl (g/day)', -0.05095530326516728), ('K (g/day)', 0.061875393019701554), ('Mg (g/day)', 0.02887785949633683)]\n",
            "Intercept:  3.0737620136490027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyAdt1gnKvnD",
        "outputId": "d52a5c98-927d-483b-ad04-3d2ab21db37f"
      },
      "source": [
        "# Model Predictions:\n",
        "\n",
        " # Apply our LR model to the test data and predict output\n",
        "predictionsLR = lrModel.transform(test_data).select(features_cols + labelColName + predictionColName)\n",
        "\n",
        " # Print the first 15 rows of your predictions\n",
        "predictionsLR.show(15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+-------------+---------------------+-----------------+--------------+----------+---------+----------+----------+---------+----------+--------+------------------+\n",
            "|Crude fiber (g/day)|Sugar (g/day)|Crude protein (g/day)|Crude fat (g/day)|Starch (g/day)|Ca (g/day)|P (g/day)|Na (g/day)|Cl (g/day)|K (g/day)|Mg (g/day)|Protein%|       predProtein|\n",
            "+-------------------+-------------+---------------------+-----------------+--------------+----------+---------+----------+----------+---------+----------+--------+------------------+\n",
            "|               4516|         1000|                 3749|              865|          5346|       137|      100|        48|       126|      374|        82|    2.44| 3.244637711442318|\n",
            "|               3704|          825|                 3090|              712|          4402|       113|       83|        40|       103|      307|        68|     2.5| 3.196474709350346|\n",
            "|               3686|          817|                 3063|              706|          4367|       112|       82|        39|       103|      305|        67|    2.52|3.1454289786640413|\n",
            "|               2555|          648|                 2356|              531|          3312|        88|       65|        31|        71|      220|        49|    2.54|3.1154588582008174|\n",
            "|               4221|          985|                 3649|              834|          5174|       134|       99|        47|       118|      355|        78|    2.54|3.1360055798009783|\n",
            "|                  0|            0|                    0|                0|             0|         0|        0|         0|         0|        0|         0|    2.55|3.0737620136490027|\n",
            "|               3317|          810|                 2971|              673|          4192|       110|       81|        39|        92|      283|        62|    2.55|3.1212686174492763|\n",
            "|               4272|         1307|                 4579|             1001|          6322|       175|      129|        62|       117|      392|        88|    2.56| 3.113852434453394|\n",
            "|               3052|          872|                 3095|              684|          4300|       117|       86|        41|        84|      274|        61|    2.57| 3.127924448646853|\n",
            "|               3697|          933|                 3397|              766|          4778|       126|       93|        44|       103|      318|        70|    2.57|3.0589450058474643|\n",
            "|               2983|          716|                 2635|              599|          3725|        97|       71|        34|        83|      253|        56|    2.58| 3.130639722988372|\n",
            "|               2924|          923|                 3283|              678|          4757|       125|       92|        42|        79|      270|        60|     2.6|3.3747090884000297|\n",
            "|               3478|         1025|                 3617|              795|          5011|       137|      101|        49|        96|      315|        70|     2.6| 3.042033705435463|\n",
            "|                  0|            0|                    0|                0|             0|         0|        0|         0|         0|        0|         0|    2.65|3.0737620136490027|\n",
            "|               4036|         1059|                 3823|              857|          5357|       143|      105|        50|       112|      352|        78|    2.65|3.1668610820548246|\n",
            "+-------------------+-------------+---------------------+-----------------+--------------+----------+---------+----------+----------+---------+----------+--------+------------------+\n",
            "only showing top 15 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FbVbCsrYC6N",
        "outputId": "9ce6a6c2-3e99-4953-e1c2-d4dffcaf919c"
      },
      "source": [
        "# Now let's compute an evaluation metric for our test dataset\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        " # Create an RMSE evaluator using the label and predicted columns\n",
        "regEval = RegressionEvaluator(predictionCol=predictionColName, labelCol=labelColName, metricName=\"rmse\")\n",
        "\n",
        " # Run the evaluator on the DataFrame\n",
        "rmse = regEval.evaluate(predictionsLR)\n",
        "\n",
        "print(\"Root Mean Squared Error: %.2f\" % rmse)\n",
        "\n",
        "# Now let's compute another evaluation metric for our test dataset\n",
        "r2 = regEval.evaluate(predictionsLR, {regEval.metricName: \"r2\"})\n",
        "\n",
        "print(\"r2: {0:.2f}\".format(r2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error: 0.24\n",
            "r2: 0.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c2p-QIoaI9c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}